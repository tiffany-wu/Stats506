---
title: "9.10.24 Vector_MonteCarlo"
format: html
editor: visual
---

```{r}
# Clear environment
rm(list = ls())
```

# From Quiz 1
```{r}
x <- c(8, 2, -8)
y <- x[x > 3]
sum(x + y)
```

# 9.10.24 Notes

## Vectorization
```{r}
## Not vectorized
x <- 0:500

s0 <- 0
for (i in seq_along(x)) {
  s0 <- s0 + x[i]
}

## Vectorized
s1 <- sum(x)  # 125250

## Do we get the same sum? Yes
s0 == s1

# Vectorization is much quicker
sum(c(2,3,4))

x <- c(5,2,6,1,2,3)
s0 <- 0

for (i in 1:length(x)){
  s0 <- s0 + x[i]
  return(s0) # ONLY works inside a function
}
s0

system.time(sum(x)) #not super useful because sum is going too fast. R's timing system can't catch this
```

## Vectorization is much faster
```{r}
# Library for timing comparison, esp. for small times
library(microbenchmark)

x <- 1:10000
microbenchmark(a = sum(x),
               b = {
                 s0 <- 0
                 for (i in seq_along(x)) {
                   s0 <- s0 + x[i]
                 }
               })
# output unit: nanoseconds
```

# The apply family of functions

```{r}
list1 <- list(c(1,2,3), c(1), c(1:10))
length(list1) # 3 things inside list
length(list1[[2]])

sapply(list1, length)

m <- matrix(c(1,2,3,1,2,3,1,2), nrow = 4)

apply(m, 2, mean) # on matrix m, for each column, calculate mean
apply(m, 1, mean) # on matrix m, for each row, calculate mean
apply(m, c(1,2), mean) # On matrix m, for each row and column, calculate mean

m <- matrix(c(1,NA,3,1,2,3,1,2), nrow = 4)
apply(m, 2, mean)
apply # ... at end. Can put in a NAMED argument that mean will accept
apply(m, 2, mean, na.rm=TRUE)

# Want mean of abs value
m <- matrix(c(1,NA,3,1,2,3,-1,2), nrow = 4)

# Nesting way-- this is confusing
apply(apply(m, c(1,2), mean), 2, mean, na.rm=TRUE)

# Function way
#' input: vector k
#' output: vector of length 1 of average abs value
abs_and_mean <- function(k, na.rm=FALSE) {
  mean(abs(k), na.rm=na.rm)
}
apply(m, 2, abs_and_mean, na.rm=T)

# Anonymous function way
apply(m, 2, function(k, na.rm=TRUE) {
  mean(abs(k), na.rm=na.rm)
})
```

A dataframe is a list of vectors that are all the same length.

lapply is for lists, applies function to every element of the list.

```{r}
list1 <- list(c(1,2,3), c(1), c(1:10))
lapply(list1, length)
# lapply returns a list

# sapply returns a vector. "s" is for simplify. If R knows how to simplify to vector, it will. Can be dangerous if list types/dimensions are unsimplifiable/compatible. 
# Only use sapply if you're certain everything will always be compatible.
sapply(list1, length) # same answer as a vector
```

```{r}
data(mtcars) #df= list of vectors the same length

trial1 <- sapply(mtcars, mean)
trial1

# vapply is even SAFER than lapply. But we have to give an example of what the output looks like or everything crashes

vapply(mtcars, mean, 1) #single number output, guarantees output is a vector
vapply(mtcars, mean, 15) # can put any number as example

vapply(mtcars, mean, c(2,2)) # vector of length 2 is the output. But using mean, it can't give this to you, so it will error.

vapply(mtcars, is.numeric, TRUE)

# USE VAPPLY IF YOU WANT FUNCTION TO STAND TEST OF TIME
```

```{r}
b <- list(1:3, 1:5)
b
a <- c(1,3) # pretend you want to pull out the first number of 1:3 for b and the third number for 1:5

# mapply if no dataframe, mixture of different data types
mapply(function(x, index){
  x[index]
}, x= b, index = a) # don't need x= or index= though
```

```{r}
data(iris)

# if you want average, grouped by type
tapply(iris$Petal.Length, iris$Species, mean)
```

But apply families hide loops inside the command, so it is NOT vectorized.

Vectorized functions are faster and more efficient, but the only way to do this is to use commands/functions already vectorized in R.

# Vectorized functions
```{r}
# Not a vectorized function

#' Generate the negative absolute value of an input
#' @param x a numeric value
#' @return the negative absolute value
negabs <- function(x) {
  if (x > 0) {
    return(-x)
  } else {
    return(x)
  }
}

negabs(5)
negabs(c(2,-3)) # this won't work because it's not vectorized

# Naive approach to vectorizaiton
#' Generate the negative absolute value of an input
#'
#' This version is naively vectorized
#' @param x a numeric value
#' @return the negative absolute value
negabs2 <- function(x) {
  for (i in seq_along(x)) { # Don't know length of vector
    if (x[i] > 0) {
      x[i] <- -x[i]
    }
  }
  return(x)
}

negabs2(3)

# Best way to vectorize
#' A fully vectorized function to generate the negative absolute
#' value of an input.
#'
#' @param x a numeric value
#' @return the negative absolute value
negabs3 <- function(x) {
  return(-abs(x))
}
negabs3(3)

negabs4 <- function(x) {
  -abs(x)
}
negabs4(3)


x <- -5000:5000
microbenchmark(negabs2(x), negabs3(x), negabs4(x)) # the simplest negabs4 wins
```

# Monte Carlo Simulation
If there's a stat you want an estimate for but you dont' have a closed-form solution, you need to do a simulation.

MC creates a lot of data, simulating a random process and then directly averaging the values of interest. Approximation is close enough.

```{r}
# First, generate artificial data
# runif draws data from UNIFORM distribution
# "pseudo"random. True randomness hard to achieve. COmputers are deterministic.
# runif uses a "seed" to generate "pseudorandom" number. Seed based on memory your computer has, ID code, screen size, the nanosecond you hit run on computer, etc.
a = runif(1)
b = runif(1)
a == b # FALSE

# R has lots of distributions you can draw from
hist(rgamma(1000, shape = 4, rate = 2))

# Sample function: sample from numbers 5 to 73, give me 10 numbers drawn without or with replacement
sample(5:73, 10, replace = TRUE)

sample(0:1, 10, replace = TRUE) # simulating coin flip

# Setting seed

set.seed(28)
a = runif(1)

set.seed(28)
b = runif(1)

a == b # TRUE

set.seed(28)
sample(0:1, 10, replace = TRUE)

set.seed(28)
sample(0:1, 10, replace = TRUE)
```

# Example 1: Estimating pi

The area of the square is 
. The area of the circle is pi
. This means that the ratio of the area of the circle to the area of the circle (area of circle/area of square) = pi/4
 
. We can therefore draw points from within the square, and use the ratio found within the circle to estimate 
.

```{r}
plot(NULL, xlim = c(-1,1), ylim = c(-1,1), asp = 1, xlab = "",
     ylab = "")
rect(-1, -1, 1, 1)
draw.circle(0, 0, 1)
```

```{r}
#' Estimates the value of pi by the proportion of points inside a square falling
#' inside a circle inscribed in that square.
#' @param n number of iterations
#' @return estimate of pi
estimate_pi <- function(n) {
  xcoord <- runif(n, -1, 1) # x coord of square
  ycoord <- runif(n, -1, 1) # y coord of square
  in_circle <- sqrt(xcoord^2 + ycoord^2) <= 1 # inside of circle, vector of TRUEs and FALSEs as output
  return(4 * sum(in_circle)/n) # estimate of pi, what percentage fall inside of circle vs. outside of circle.
}

estimate_pi(10000)
```

```{r}
# We can re-run it several times to look at the distribution of estimates to see the uncertainty in the Monte Carlo methods:

reps <- 100
n2 <- vector(length = reps)
for (i in seq_len(reps)) {
  n2[i] <- estimate_pi(100)
}
n4 <- vector(length = reps)
for (i in seq_len(reps)) {
  n4[i] <- estimate_pi(10000)
}
n6 <- vector(length = reps)
for (i in seq_len(reps)) {
  n6[i] <- estimate_pi(1000000)
}

boxplot(data.frame(n2, n4, n6))
abline(h = pi, col = "red")
```


# 9.12.24 Notes

Monte Carlo simulations for when you want to find info on things without a closed form/no moments/not a standard distribution.

## Basic Example 2 - Estimating percentiles

Let’s estimate percentiles for *t-distributions* with various degrees of freedom.

```{r}
#First, let’s draw a random sample from the distribution

set.seed(100)
n <- 10000 # Number of samples
df <- 3 # Degrees of freedom
sim <- rt(n, df) # Draw from t-distribution w/ 3 dfs

# histogram -- looks weird b/c t-distribution has heavy tails with small dfs. Had extreme values.
hist(sim)
```

What percentage of values from our simulation fall outside standard z-score bounds? (between -1.96 to 1.96)
```{r}
# Should be around 5% if a true z-score
mean(sim < -1.96)
mean(sim > 1.96)
```

What critical value do I need to match a certain percentile? 

If our true distribution is a t-dist w/ 3 dfs, these are the critical values we would need:
```{r}
# Order the numbers from our simulation
ordered_sim <- sim[order(sim)]
head(ordered_sim) # prints out first couple entries

ordered_sim[round(.025*n)]
ordered_sim[round(.975*n)]
```

## More advanced percentile example
```{r}
mcrep <- 10000                  # Simulation replications
n <- 30                         # Sample size we are studying
sim <- rexp(n * mcrep,          # Simulate standard exponential data
            rate = 1)           # mean = 1/rate
xmat <- matrix(sim,             # Reshape to matrix. Each column is like 1 random draw of data in the world. Usually in real world, we only get 1 draw.
               ncol = mcrep)
mn <- apply(xmat, 2, mean)      # Sample mean of each column (replicate), 2 = column
std <- apply(xmat, 2, sd)       # Sample SD of each column (replicate)
se <- std / sqrt(n)             # Standard errors of the mean
m <- qt(1 - (1 - .95) / 2,      # qt gets your quantiles for your distribution. This gives you the lower tail by default.
        df = n - 1)             # Multiplier for confidence level
lcb <- mn - m * se              # lower confidence bounds
ucb <- mn + m * se              # upper confidence bounds

# Coverage probability is then the proportion of confidence intervals which cover the mean of 1.

cvrg <- sum(lcb < 1 & ucb > 1)/mcrep
cvrg

# What should be 95% coverage yields only 92.7%.
```

## Broadcasting

Recycling.

```{r}
# The 1 and 10 will repeat
c(1, 10) + c(2, 2, 6, 6)

# This has warning, but still does recycling
c(1, 10) + c(2, 2, 2)
```

For Matrices:
Matrices in R are column-dominant, so columns get filled in first.

```{r}
m <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8), nrow = 2)
m

m + 1 # recycling works here, 1 applies to every element of the matrix

m * c(1, 10)

# If you want to center each column:
m_means <- apply(m, 2, mean)
m_means
m - m_means # this wrong
rep(m_means, each = 2) # to get around the recycling, but this is not v efficient

# How to convert a column-dominant to a row-dominant matrix? Transpose it
t(m) - m_means

t(t(m)-m_means)

# rowMeans will give you the means of each row, but still ahve to transpose
m
rowMeans(m)

# rowMeans will be FASTER than any apply function
```

## Correlation Coefficients

Pretend you have ~1000 predictors but only 30 n's and want to use correlations to pick which predictors to include.

Don't do this IRL. Just do a penalized regression instead.

```{r}
# Generate fake correlation matrix
n <- 30
m <- 1000
r <- 0.4 # This is the expected correlation
y <- rnorm(n)
# When you're generating correlations
xmat <- matrix(rep(y, times = m), ncol = m)
# Reduce the variance in X by the amount added from correlation with y, rnorm provides noise
xmat <- r * xmat + rnorm(n * m, sd = sqrt(1 - r^2))
```

```{r}
library(microbenchmark)
# First approach, calculate as a loop
mb1 <- microbenchmark(loop = {
  r1 <- vector(length = m)
  for (i in seq_len(m)) {
    r1[i] <- cor(xmat[, i], y)
  }
})

mb2 <- microbenchmark(apply = {
  r2 <- apply(xmat, 2, function(v) cor(v, y))
})

all.equal(r1, r2)

mb3 <- microbenchmark(linalg = {
  rmn <- colMeans(xmat)
  xmat_c <- xmat - matrix(rep(rmn, each = n), ncol = m)
  rsd <- apply(xmat, 2, sd)
  xmat_s <- xmat_c / matrix(rep(rsd, each = n), ncol = m)
  y_s <- (y - mean(y)) / sd(y)
  r3 <- y_s %*% xmat_s / (n - 1)
  r3 <- as.vector(r3)
})

all.equal(r1, r3)

mb4 <- microbenchmark(linalg_b = {
  rmn <- colMeans(xmat)
  xmat_c <- t(t(xmat) - rmn)
  rvar <- colSums(xmat_c^2) / (dim(xmat)[1] - 1)
  rsd <- sqrt(rvar)
  xmat_s <- t(t(xmat_c) / rsd)
  y_s <- (y - mean(y)) / sd(y)
  r4 <- y_s %*% xmat_s / (n - 1)
  r4 <- as.vector(r4)
})

all.equal(r1, r4)

rbind(mb1, mb2, mb3, mb4)

# The fourth approach using linear algebra and broadcasting is by far the most efficient here.
```

## Functional programming

R is an object-oriented program. Others are functional programs. R can kind of do some functional programs because functions can be objects in R.

```{r}

```


# 9.17.24 Notes

## Debugging functions

### Print statement
Use with paste so you can keep track of everything you're printing.
```{r}
#' Calculate sample skewness
#' @param x a numeric vector
#' @return skewness
skewness <- function(x) {
  xbar <- mean(x)
  print(paste("xbar:", xbar))
  xc <- x - xbar
  print(paste("xc:", paste(xc, collapse = ", ")))
  num <- sum(xc^3)
  denom <- sum(xc^2)
  print(paste("Num:", num))
  print(paste("Denom:", denom))
  return(num/(denom^(3/2)))
}
skewness(sample(1:100, 10, TRUE))
```

### Double arrow

If you're printing too many times, that's probably not the best way to debug.

Can use double arrow to put things into global environment. BUT you'll need to change it back to single arrow or function will throw an error.

```{r}
#' Calculate sample skewness
#' @param x a numeric vector
#' @return skewness
skewness <- function(x) {
  xbar <- mean(x)
  print(paste("xbar:", xbar))
  xc <- x - xbar
  print(paste("xc:", paste(xc, collapse = ", ")))
  num <- sum(xc^3)
  denom <- sum(xc^2)
#  print(paste("Num:", num))
#  print(paste("Denom:", denom))
  return(num/(denom^(3/2)))
}
skewness(sample(1:100, 10, TRUE))
```

## The best way to debug
### THE BROWSER

Have to take the browser out after you use it for the function to run though.

Don't put browser inside a for-loop, or you'll get stuck. Put it RIGHT BEFORE the for loop begins.

```{r}
#' Calculate sample skewness
#' @param x a numeric vector
#' @return skewness
skewness <- function(x) {
  xbar <- mean(x)
  print(paste("xbar:", xbar))
  xc <- x - xbar
 # browser()
  print(paste("xc:", paste(xc, collapse = ", ")))
  num <- sum(xc^3)
  denom <<- sum(xc^2)
#  print(paste("Num:", num))
#  print(paste("Denom:", denom))
  return(num/(denom^(3/2)))
}

skewness(sample(1:100, 10, TRUE))
```

### IF you want to start a browser whenever
```{r}
debug(summarize)
#summarize(sample(1:100, 10, TRUE))
#undebug(summarize)
```

```{r}
foo <- function() {
  bar()
}

bar <- function() {
  baz()
}

baz <- function() {
  print(1)
}

foo()

#debug(foo)
foo()
```

### Traceback

4: stop("error!") at #2
3: baz() at #2
2: bar() at #2
1: foo()

The "at #2" stands for the second line of baz function.
```{r}
foo <- function() {
  bar()
}

bar <- function() {
  baz()
}

baz <- function() {
  stop("error!")
}

foo()

traceback() # Shows callstack
```

