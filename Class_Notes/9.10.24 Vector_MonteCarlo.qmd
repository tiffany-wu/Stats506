---
title: "9.10.24 Vector_MonteCarlo"
format: html
editor: visual
---

```{r}
# Clear environment
rm(list = ls())
```

# From Quiz 1
```{r}
x <- c(8, 2, -8)
y <- x[x > 3]
sum(x + y)
```

# 9.10.24 Notes

## Vectorization
```{r}
## Not vectorized
x <- 0:500

s0 <- 0
for (i in seq_along(x)) {
  s0 <- s0 + x[i]
}

## Vectorized
s1 <- sum(x)  # 125250

## Do we get the same sum? Yes
s0 == s1

# Vectorization is much quicker
sum(c(2,3,4))

x <- c(5,2,6,1,2,3)
s0 <- 0

for (i in 1:length(x)){
  s0 <- s0 + x[i]
  return(s0) # ONLY works inside a function
}
s0

system.time(sum(x)) #not super useful because sum is going too fast. R's timing system can't catch this
```

## Vectorization is much faster
```{r}
# Library for timing comparison, esp. for small times
library(microbenchmark)

x <- 1:10000
microbenchmark(a = sum(x),
               b = {
                 s0 <- 0
                 for (i in seq_along(x)) {
                   s0 <- s0 + x[i]
                 }
               })
# output unit: nanoseconds
```

# The apply family of functions

```{r}
list1 <- list(c(1,2,3), c(1), c(1:10))
length(list1) # 3 things inside list
length(list1[[2]])

sapply(list1, length)

m <- matrix(c(1,2,3,1,2,3,1,2), nrow = 4)

apply(m, 2, mean) # on matrix m, for each column, calculate mean
apply(m, 1, mean) # on matrix m, for each row, calculate mean
apply(m, c(1,2), mean) # On matrix m, for each row and column, calculate mean

m <- matrix(c(1,NA,3,1,2,3,1,2), nrow = 4)
apply(m, 2, mean)
apply # ... at end. Can put in a NAMED argument that mean will accept
apply(m, 2, mean, na.rm=TRUE)

# Want mean of abs value
m <- matrix(c(1,NA,3,1,2,3,-1,2), nrow = 4)

# Nesting way-- this is confusing
apply(apply(m, c(1,2), mean), 2, mean, na.rm=TRUE)

# Function way
#' input: vector k
#' output: vector of length 1 of average abs value
abs_and_mean <- function(k, na.rm=FALSE) {
  mean(abs(k), na.rm=na.rm)
}
apply(m, 2, abs_and_mean, na.rm=T)

# Anonymous function way
apply(m, 2, function(k, na.rm=TRUE) {
  mean(abs(k), na.rm=na.rm)
})
```

A dataframe is a list of vectors that are all the same length.

lapply is for lists, applies function to every element of the list.

```{r}
list1 <- list(c(1,2,3), c(1), c(1:10))
lapply(list1, length)
# lapply returns a list

# sapply returns a vector. "s" is for simplify. If R knows how to simplify to vector, it will. Can be dangerous if list types/dimensions are unsimplifiable/compatible. 
# Only use sapply if you're certain everything will always be compatible.
sapply(list1, length) # same answer as a vector
```

```{r}
data(mtcars) #df= list of vectors the same length

trial1 <- sapply(mtcars, mean)
trial1

# vapply is even SAFER than lapply. But we have to give an example of what the output looks like or everything crashes

vapply(mtcars, mean, 1) #single number output, guarantees output is a vector
vapply(mtcars, mean, 15) # can put any number as example

vapply(mtcars, mean, c(2,2)) # vector of length 2 is the output. But using mean, it can't give this to you, so it will error.

vapply(mtcars, is.numeric, TRUE)

# USE VAPPLY IF YOU WANT FUNCTION TO STAND TEST OF TIME
```

```{r}
b <- list(1:3, 1:5)
b
a <- c(1,3) # pretend you want to pull out the first number of 1:3 for b and the third number for 1:5

# mapply if no dataframe, mixture of different data types
mapply(function(x, index){
  x[index]
}, x= b, index = a) # don't need x= or index= though
```

```{r}
data(iris)

# if you want average, grouped by type
tapply(iris$Petal.Length, iris$Species, mean)
```

But apply families hide loops inside the command, so it is NOT vectorized.

Vectorized functions are faster and more efficient, but the only way to do this is to use commands/functions already vectorized in R.

# Vectorized functions
```{r}
# Not a vectorized function

#' Generate the negative absolute value of an input
#' @param x a numeric value
#' @return the negative absolute value
negabs <- function(x) {
  if (x > 0) {
    return(-x)
  } else {
    return(x)
  }
}

negabs(5)
negabs(c(2,-3)) # this won't work because it's not vectorized

# Naive approach to vectorizaiton
#' Generate the negative absolute value of an input
#'
#' This version is naively vectorized
#' @param x a numeric value
#' @return the negative absolute value
negabs2 <- function(x) {
  for (i in seq_along(x)) { # Don't know length of vector
    if (x[i] > 0) {
      x[i] <- -x[i]
    }
  }
  return(x)
}

negabs2(3)

# Best way to vectorize
#' A fully vectorized function to generate the negative absolute
#' value of an input.
#'
#' @param x a numeric value
#' @return the negative absolute value
negabs3 <- function(x) {
  return(-abs(x))
}
negabs3(3)

negabs4 <- function(x) {
  -abs(x)
}
negabs4(3)


x <- -5000:5000
microbenchmark(negabs2(x), negabs3(x), negabs4(x)) # the simplest negabs4 wins
```

# Monte Carlo Simulation
If there's a stat you want an estimate for but you dont' have a closed-form solution, you need to do a simulation.

MC creates a lot of data, simulating a random process and then directly averaging the values of interest. Approximation is close enough.

```{r}
# First, generate artificial data
# runif draws data from UNIFORM distribution
# "pseudo"random. True randomness hard to achieve. COmputers are deterministic.
# runif uses a "seed" to generate "pseudorandom" number. Seed based on memory your computer has, ID code, screen size, the nanosecond you hit run on computer, etc.
a = runif(1)
b = runif(1)
a == b # FALSE

# R has lots of distributions you can draw from
hist(rgamma(1000, shape = 4, rate = 2))

# Sample function: sample from numbers 5 to 73, give me 10 numbers drawn without or with replacement
sample(5:73, 10, replace = TRUE)

sample(0:1, 10, replace = TRUE) # simulating coin flip

# Setting seed

set.seed(28)
a = runif(1)

set.seed(28)
b = runif(1)

a == b # TRUE

set.seed(28)
sample(0:1, 10, replace = TRUE)

set.seed(28)
sample(0:1, 10, replace = TRUE)
```

# Example 1: Estimating pi

The area of the square is 
. The area of the circle is pi
. This means that the ratio of the area of the circle to the area of the circle (area of circle/area of square) = pi/4
 
. We can therefore draw points from within the square, and use the ratio found within the circle to estimate 
.

```{r}
plot(NULL, xlim = c(-1,1), ylim = c(-1,1), asp = 1, xlab = "",
     ylab = "")
rect(-1, -1, 1, 1)
draw.circle(0, 0, 1)
```

```{r}
#' Estimates the value of pi by the proportion of points inside a square falling
#' inside a circle inscribed in that square.
#' @param n number of iterations
#' @return estimate of pi
estimate_pi <- function(n) {
  xcoord <- runif(n, -1, 1) # x coord of square
  ycoord <- runif(n, -1, 1) # y coord of square
  in_circle <- sqrt(xcoord^2 + ycoord^2) <= 1 # inside of circle, vector of TRUEs and FALSEs as output
  return(4 * sum(in_circle)/n) # estimate of pi, what percentage fall inside of circle vs. outside of circle.
}

estimate_pi(10000)
```

```{r}
# We can re-run it several times to look at the distribution of estimates to see the uncertainty in the Monte Carlo methods:

reps <- 100
n2 <- vector(length = reps)
for (i in seq_len(reps)) {
  n2[i] <- estimate_pi(100)
}
n4 <- vector(length = reps)
for (i in seq_len(reps)) {
  n4[i] <- estimate_pi(10000)
}
n6 <- vector(length = reps)
for (i in seq_len(reps)) {
  n6[i] <- estimate_pi(1000000)
}

boxplot(data.frame(n2, n4, n6))
abline(h = pi, col = "red")
```


# 9.12.24 Notes

Monte Carlo simulations for when you want to find info on things without a closed form/no moments/not a standard distribution.

## Basic Example 2 - Estimating percentiles

Let’s estimate percentiles for *t-distributions* with various degrees of freedom.

```{r}
#First, let’s draw a random sample from the distribution

set.seed(100)
n <- 10000 # Number of samples
df <- 3 # Degrees of freedom
sim <- rt(n, df) # Draw from t-distribution w/ 3 dfs

# histogram -- looks weird b/c t-distribution has heavy tails with small dfs. Had extreme values.
hist(sim)
```

What percentage of values from our simulation fall outside standard z-score bounds? (between -1.96 to 1.96)
```{r}
# Should be around 5% if a true z-score
mean(sim < -1.96)
mean(sim > 1.96)
```

What critical value do I need to match a certain percentile? 

If our true distribution is a t-dist w/ 3 dfs, these are the critical values we would need:
```{r}
# Order the numbers from our simulation
ordered_sim <- sim[order(sim)]
head(ordered_sim) # prints out first couple entries

ordered_sim[round(.025*n)]
ordered_sim[round(.975*n)]
```

## More advanced percentile example
```{r}
mcrep <- 10000                  # Simulation replications
n <- 30                         # Sample size we are studying
sim <- rexp(n * mcrep,          # Simulate standard exponential data
            rate = 1)           # mean = 1/rate
xmat <- matrix(sim,             # Reshape to matrix. Each column is like 1 random draw of data in the world. Usually in real world, we only get 1 draw.
               ncol = mcrep)
mn <- apply(xmat, 2, mean)      # Sample mean of each column (replicate), 2 = column
std <- apply(xmat, 2, sd)       # Sample SD of each column (replicate)
se <- std / sqrt(n)             # Standard errors of the mean
m <- qt(1 - (1 - .95) / 2,      # qt gets your quantiles for your distribution. This gives you the lower tail by default.
        df = n - 1)             # Multiplier for confidence level
lcb <- mn - m * se              # lower confidence bounds
ucb <- mn + m * se              # upper confidence bounds

# Coverage probability is then the proportion of confidence intervals which cover the mean of 1.

cvrg <- sum(lcb < 1 & ucb > 1)/mcrep
cvrg

# What should be 95% coverage yields only 92.7%.
```

## Broadcasting

Recycling.

```{r}
# The 1 and 10 will repeat
c(1, 10) + c(2, 2, 6, 6)

# This has warning, but still does recycling
c(1, 10) + c(2, 2, 2)
```

For Matrices:
Matrices in R are column-dominant, so columns get filled in first.

```{r}
m <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8), nrow = 2)
m

m + 1 # recycling works here, 1 applies to every element of the matrix

m * c(1, 10)

# If you want to center each column:
m_means <- apply(m, 2, mean)
m_means
m - m_means # this wrong
rep(m_means, each = 2) # to get around the recycling, but this is not v efficient

# How to convert a column-dominant to a row-dominant matrix? Transpose it
t(m) - m_means

t(t(m)-m_means)

# rowMeans will give you the means of each row, but still ahve to transpose
m
rowMeans(m)

# rowMeans will be FASTER than any apply function
```

## Correlation Coefficients

Pretend you have ~1000 predictors but only 30 n's and want to use correlations to pick which predictors to include.

Don't do this IRL. Just do a penalized regression instead.

```{r}
# Generate fake correlation matrix
n <- 30
m <- 1000
r <- 0.4 # This is the expected correlation
y <- rnorm(n)
# When you're generating correlations
xmat <- matrix(rep(y, times = m), ncol = m)
# Reduce the variance in X by the amount added from correlation with y, rnorm provides noise
xmat <- r * xmat + rnorm(n * m, sd = sqrt(1 - r^2))
```

```{r}
library(microbenchmark)
# First approach, calculate as a loop
mb1 <- microbenchmark(loop = {
  r1 <- vector(length = m)
  for (i in seq_len(m)) {
    r1[i] <- cor(xmat[, i], y)
  }
})

mb2 <- microbenchmark(apply = {
  r2 <- apply(xmat, 2, function(v) cor(v, y))
})

all.equal(r1, r2)

mb3 <- microbenchmark(linalg = {
  rmn <- colMeans(xmat)
  xmat_c <- xmat - matrix(rep(rmn, each = n), ncol = m)
  rsd <- apply(xmat, 2, sd)
  xmat_s <- xmat_c / matrix(rep(rsd, each = n), ncol = m)
  y_s <- (y - mean(y)) / sd(y)
  r3 <- y_s %*% xmat_s / (n - 1)
  r3 <- as.vector(r3)
})

all.equal(r1, r3)

mb4 <- microbenchmark(linalg_b = {
  rmn <- colMeans(xmat)
  xmat_c <- t(t(xmat) - rmn)
  rvar <- colSums(xmat_c^2) / (dim(xmat)[1] - 1)
  rsd <- sqrt(rvar)
  xmat_s <- t(t(xmat_c) / rsd)
  y_s <- (y - mean(y)) / sd(y)
  r4 <- y_s %*% xmat_s / (n - 1)
  r4 <- as.vector(r4)
})

all.equal(r1, r4)

rbind(mb1, mb2, mb3, mb4)

# The fourth approach using linear algebra and broadcasting is by far the most efficient here.
```

## Functional programming

R is an object-oriented program. Others are functional programs. R can kind of do some functional programs because functions can be objects in R.

```{r}

```


# 9.17.24 Notes

## Debugging functions

### Print statement
Use with paste so you can keep track of everything you're printing.
```{r}
#' Calculate sample skewness
#' @param x a numeric vector
#' @return skewness
skewness <- function(x) {
  xbar <- mean(x)
  print(paste("xbar:", xbar))
  xc <- x - xbar
  print(paste("xc:", paste(xc, collapse = ", ")))
  num <- sum(xc^3)
  denom <- sum(xc^2)
  print(paste("Num:", num))
  print(paste("Denom:", denom))
  return(num/(denom^(3/2)))
}
skewness(sample(1:100, 10, TRUE))
```

### Double arrow

If you're printing too many times, that's probably not the best way to debug.

Can use double arrow to put things into global environment. BUT you'll need to change it back to single arrow or function will throw an error.

```{r}
#' Calculate sample skewness
#' @param x a numeric vector
#' @return skewness
skewness <- function(x) {
  xbar <- mean(x)
  print(paste("xbar:", xbar))
  xc <- x - xbar
  print(paste("xc:", paste(xc, collapse = ", ")))
  num <- sum(xc^3)
  denom <- sum(xc^2)
#  print(paste("Num:", num))
#  print(paste("Denom:", denom))
  return(num/(denom^(3/2)))
}
skewness(sample(1:100, 10, TRUE))
```

## The best way to debug
### THE BROWSER

Have to take the browser out after you use it for the function to run though.

Don't put browser inside a for-loop, or you'll get stuck. Put it RIGHT BEFORE the for loop begins.

```{r}
#' Calculate sample skewness
#' @param x a numeric vector
#' @return skewness
skewness <- function(x) {
  xbar <- mean(x)
  print(paste("xbar:", xbar))
  xc <- x - xbar
 # browser()
  print(paste("xc:", paste(xc, collapse = ", ")))
  num <- sum(xc^3)
  denom <<- sum(xc^2)
#  print(paste("Num:", num))
#  print(paste("Denom:", denom))
  return(num/(denom^(3/2)))
}

skewness(sample(1:100, 10, TRUE))
```

### IF you want to start a browser whenever
```{r}
debug(summarize)
#summarize(sample(1:100, 10, TRUE))
#undebug(summarize)
```

```{r}
foo <- function() {
  bar()
}

bar <- function() {
  baz()
}

baz <- function() {
  print(1)
}

foo()

#debug(foo)
foo()
```

### Traceback

4: stop("error!") at #2
3: baz() at #2
2: bar() at #2
1: foo()

The "at #2" stands for the second line of baz function.
```{r}
foo <- function() {
  bar()
}

bar <- function() {
  baz()
}

baz <- function() {
  stop("error!")
}

foo()

traceback() # Shows callstack
```

# 9.19.24 Notes

## Quiz 2
```{r}
a <- c(1, 10, 100)
b <- matrix(1:12, ncol = 3)
a
b
a+b
a + t(b)
c <- t(a + t(b)) # or t(t(b) + a)
c

# Review
# Vector operations
c(1,2) + c(2,3,4) # By default, element by element operations, will RECYCLE
# Length will be length of LONGER object = length 3

c(1,2) + mean(c(1,2,3)) # length = 2 because mean spits out 1 number, vector of length 1

# Matrices
matrix(1:6, nrow= 2)
c(1,2) * matrix(1:6, nrow= 2) # vectors are DIRECTIONLESS
# Go multiply down columns, same way matrix is defined

# 1x2 times 2x3 = 1x3 matrix
matrix(c(1,2), nrow = 1) %*% matrix(1:6, nrow = 2)

# Can do this to check dimentions
if(all(dim(result != c(2,3)))){
  stop()
}
```

## Debugging what part is slow: Profiling
```{r}
library(profvis)

# This runs profiling and produces measurements.Takes snapshots. But only good for longer-running code.
# The command is jsut (profvis)
```

## Regressions
~ is a FORMULA
```{r}
3 ~ 2

ff <- 3~2
typeof(ff) # language object. Basically like a sentence, with left and right side of equation

b~4 # like b = 4 math statement, but won't save to global environment. Just specifies the equation.
```

```{r}
data(mtcars)

form <- qsec ~ disp + cyl # This is just a formula
# qsec = beta_0 + beta_1*disp + beta_2*cyl + epsilon
# RHS: disp + cyl
# LHS: qsec

# linear regression
lm(form, data = mtcars)
lm(qsec~disp + cyl, data = mtcars)

# What if you don't want intercept? Probably won't ever use this
lm(qsec~disp + cyl + 0, data = mtcars)
lm(qsec~disp + cyl -1, data = mtcars)

# Interactions
lm(qsec~disp*cyl, data = mtcars)
lm(qsec~disp:cyl, data = mtcars) # This assumes the main effects = 0
lm(qsec~disp + cyl + disp:cyl, data = mtcars) # Same as just the disp*cyl regression, if you want to build in manually

# Include EVERYTHING in dataset
lm(qsec~., data = mtcars)

# Include everything EXCEPT am
lm(qsec~.-am, data = mtcars)

# Everything in R is an Object, save lm object
mod <- lm(qsec~ . - am, data = mtcars)

summary(mod) # Get all the useful info, slower than just running lm so it's not automatic

# Save summary as an object
smod <- summary(mod)
is(mod) # "lm"       "oldClass"
is(smod) # summary.lm

# These can just be lists
class(mod) <- "list"

# Entire code
stats:::print.lm(mod)

# Can pull things out
mod$coefficients # Just betas
smod$coefficients # betas, SEs, t-val, p-value

coefficients(mod) # If you have other things in your mod and just want to make sure you pull out the coefficients

# Don't want to write special code for lm vs glm vs IRT vs mixed models, etc. "Predict" will work with almost every single model, not just lm. mod$fitted.values might not work with glm/IRT/mixed/etc.
head(predict(mod)) # y-predictions, same as fitted values. 
head(mod$fitted.values) # y-predictions

smod$r.squared # single-length vector of the R-squared value


```

## Polynomials
```{r}
lm(mpg ~ wt*wt, data = mtcars) # This won't work for quadratic. R will automatically avoid duplication issue

# Method 1 for poly: Manual creation -- NOT GOOD
mtcars$wtsq <- mtcars$wt^2

lm(mpg ~ wt + wtsq, data = mtcars)
# Don't do this because R wont' know wt and wtsq are related

# Method 2 for poly: I function
## I tells R to do math, then put it into formula
lm(mpg ~ I(wt*wt), data = mtcars) # But doesn't keep wt , just does wt squared

# Method 3: poly
lm(mpg ~ poly(wt, degree = 2), data = mtcars)
# Poly STANDARDIZES for you automatically. Some people think that you have to always convert to Z-score if you're doing polynomials. THIS IS NOT TRUE. 

# Method 4: poly with raw = TRUE
lm(mpg ~ poly(wt, degree = 2, raw = TRUE), data = mtcars)

mod2 <- lm(mpg ~ poly(wt, degree = 2, raw = TRUE) + qsec, data = mtcars)
summary(mod2)
# Only linear reg has a closed form solution. Most other models DO NOT have closed form solution, need to optimize. This is when it makes sense to standardize. Theoretically, you don't need to. But it might make convergence hard for the computers.
```

## Get the X matrix out for (XtX)^-1Xty

```{r}
# Can do it manually

X <- data.frame(intercept = rep(1, nrow(mtcars)),
                disp = mtcars$disp,
                cyl = mtcars$cyl)

# model.matrix = will give design matrix automatically
model.matrix(mod, data = mtcars) # Recommend this one
model.matrix(form, data = mtcars) # Can also pass in a formula instead of mod, but it won't work if you're subsetting your data you'll need to make another dataframe
```

## Categorical variables
```{r}
# Factors
vec <- c(10,20,20)
fvec <- as.factor(vec)
fvec
as.numeric(fvec) # 1 2 2, values as numbers, assigning labels to the numbers

mtcars$cylf <- as.factor(mtcars$cyl)

lm(mpg ~ cylf, data = mtcars)
# Intecept = average mpg for reference categories (cyl4)
# cylf6 = 8 cylinder cars have about ~15 mpg

model.matrix(~cylf, data = mtcars) # dont' care about y for design matrix so can run it without y
```

## model.frame
```{r}
model.matrix(mod, data = mtcars)

head(model.frame(mod, data= mtcars)) # does the same thing as model.matrix, but output is a data frame
# DROPS unnecessary variables, creates minimal version. So if you do the interaction, it won't include the interaction, just the variables needed for the interaction
```

## Post-estimation
Like if you want to calculate robust standard errors or marginal effects (linear combinations of predictors, what happens when we combine them together?). E.g. test whether inclusion of 1 var is diff from another (Is cylf6 different from cylf8? Or is it jsut idfferent from reference?)

```{r}
mod3 <- lm(mpg ~ cylf, data = mtcars)
```









