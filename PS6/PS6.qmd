---
title: "PS6"
author: "Tiffany Wu"
format: 
  html:
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
output: html_document
editor: source
mainfont: Times New Roman
---

## Github Repo Link to Problem Set 6:
[https://github.com/tiffany-wu/Stats506/tree/main/PS6](https://github.com/tiffany-wu/Stats506/tree/main/PS6) 

## Stratified Bootstrapping

If a sample has a categorical variable with small groups, bootstrapping can be tricky. Consider a situation where n = 100, but there is some categorical variable g where category g = 1 has only 2 observations. In a single bootstrap resample of that data, there is a chance that the bootstrap sample does not include either observation from g = 1. This implies that if we are attempting to obtain a bootstrap estimate in group g = 1, 13% of the bootstrapped samples will have no observations from that group and thus unable to produce an estimate.

A way around this is to carry out *stratified bootstrap*: Instead of taking a sample with replacement of the whole sample, take separate bootstrap resamples within each strata, then combine those resamples to generate the bootstrap sample.

Use the “lahman” data that we first introduced in sql. In the statistical analysis of baseball statistics, one metric used to measure a players performance is their Range Factor:

### a. Calculate the average RF for each team in the Fielding table. Then, since we don’t have a closed form for the standard deviation of this statistic, carry out a stratified bootstrap by team to estimate it. Do this out three ways:


```{r}
# Load necessary libraries
library(DBI) # necessary evil. Holds SQL to R, handles backend so they can talk to each other
library(RSQLite)
library(tidyverse)
library(parallel)
library(future)

lahman <- dbConnect(SQLite(), "lahman_1871-2022.sqlite")
# Doesn't really read in data, just load in connection, unlike reading in a .csv.

# What tables are available?
dbListTables(lahman)

# Find variables we need to calculate Range Factor
dbListFields(lahman, "Fielding")

# Extract Fielding table
fielding <- dbReadTable(lahman, "Fielding")

# Examine data
psych::describe(fielding)

# Compute equation (RF)
fielding2 <- fielding %>%
  mutate(RF = 3*(PO + A) / InnOuts) %>%
  filter(InnOuts != 0) %>% # Denominator can't be 0
  drop_na(RF) # There are some NAs

# Calculate average RF per team
team_rf <- fielding2 %>%
  group_by(teamID) %>%
  summarise(mean_RF = mean(RF, na.rm = TRUE)) %>%
  ungroup()

```

Do this out three ways:

#### 1. Without any parallel processing

```{r}
# Function for bootstrapping

#' Perform Stratified Bootstrapping for Grouped Data
#'
#' This function performs stratified bootstrapping on a dataset, calculating the group-level means and their associated standard errors (SE) across bootstrap replications. It also computes the actual mean for each group using the full dataset.
#'
#' @param data A data frame containing the dataset for bootstrapping. Must include columns `teamID` and `RF`, based on fielding2 dataframe.
#' 
#' @param n_bootstrap An integer specifying the number of bootstrap replications. Default is 1000. Can change to less when testing.
#'
#' @return A data frame with the following columns:
#'   - `teamID`: The group identifier.
#'   - `mean_RF`: The actual mean of `RF` for each group using the full dataset.
#'   - `SE_RF`: The standard error of the group-level mean calculated from the bootstrap replications.
#'
stratified_bootstrap <- function(data, n_bootstrap = 1000) {
  
  # Perform bootstrap sampling across the entire dataset
  bootstrap_samples <- replicate(n_bootstrap, {
    # Sample with replacement from the entire dataset
    data %>% sample_frac(replace = TRUE)
  }, simplify = FALSE)
  
  # Calculate group-level means for each bootstrap sample
  bootstrap_means <<- lapply(bootstrap_samples, function(sample_data) {
    sample_data %>%
      group_by(teamID) %>%
      summarise(mean_RF = mean(RF, na.rm = TRUE)) %>%
      ungroup()
  })
  
  # Combine all bootstrap means into a single dataframe
  combined_results <- bind_rows(bootstrap_means, .id = "replication")
  
  # Calculate the overall mean and standard error (SE) for each group
  bootstrap_results <- combined_results %>%
    group_by(teamID) %>%
    mutate(SE_RF = sd(mean_RF)) %>%
    ungroup() %>%
    select(teamID, SE_RF) %>%
    distinct(teamID, .keep_all = TRUE)
  
  # Calculate the actual mean RF for each team -- doesn't need bootstraps, use entire original dataset
  actual_means <- data %>%
      group_by(teamID) %>%
      summarise(mean_RF = mean(RF, na.rm = TRUE)) %>%
      ungroup()
  
  # Join SEs with the actual means
  bootstrap_results <- actual_means %>% 
    left_join(bootstrap_results, by = "teamID")
  
  return(bootstrap_results)
}

# Set seed for reproducibility
set.seed(2111)

# 1. Standard Approach
time_standard <- system.time({
  results_standard <- stratified_bootstrap(fielding2)
})

time_standard
# results_standard # commenting out for rendering because we'll put all results in end
```

#### 2. Using parallel processing with the parallel package.

Use the parallel package to split the data by teamID and perform the bootstrap computation for each group in parallel across multiple cores.

```{r}
# Set seed for reproducibility
set.seed(2111)

# Parallel Approach
time_parallel <- system.time({
  
  # Create cluster
  cl <- makeCluster(detectCores() - 3)
  
  # Load necessary library on workers
  clusterEvalQ(cl, library(dplyr))
  
  # Export required data and functions to the workers
  clusterExport(cl, varlist = c("fielding2", "stratified_bootstrap"))
  
  # Perform bootstrap for all replications in parallel
  bootstrap_samples <- parLapply(cl, 1:1000, function(rep) {
    library(dplyr) # Needed inside the cluster
    # Sample with replacement for the entire dataset
    sampled_data <- fielding2 %>% sample_frac(replace = TRUE)
    
    # Calculate group-level means for this bootstrap sample
    sampled_data %>%
      group_by(teamID) %>%
      summarise(mean_RF = mean(RF, na.rm = TRUE), .groups = "drop")
  })
  
  # Stop the cluster
  stopCluster(cl)
  
  # Combine all bootstrap results into one dataframe
  combined_results <- bind_rows(bootstrap_samples, .id = "replication")
  
  # Calculate the overall mean and standard error (SE) for each group
  results_parallel <- combined_results %>%
    group_by(teamID) %>%
    summarise(
      SE_RF = sd(mean_RF, na.rm = TRUE),  # Standard error from bootstrap samples
      .groups = "drop"
    )
  
  # Calculate the actual mean RF for each team using the full dataset
  actual_means <- fielding2 %>%
    group_by(teamID) %>%
    summarise(mean_RF = mean(RF, na.rm = TRUE), .groups = "drop")
  
  # Combine actual means and standard errors
  results_table_parallel <- actual_means %>%
    left_join(results_parallel, by = "teamID")
})

# Print timing and results
print(time_parallel)
# results_table_parallel
```

#### 3. Using futures with the future package.

```{r, warning = FALSE}
# Set seed for reproducibility
set.seed(2111)

# Plan multisession for future
plan(multisession, workers = detectCores() - 3)

# Futures Approach
time_future <- system.time({
  
  # Use `future` to parallelize the bootstrap sampling
  future_samples <- lapply(1:1000, function(rep) {
    future({
      # Sample with replacement for the entire dataset
      sampled_data <- fielding2 %>% sample_frac(replace = TRUE)
      
      # Calculate group-level means for this bootstrap sample
      sampled_data %>%
        group_by(teamID) %>%
        summarise(mean_RF = mean(RF, na.rm = TRUE), .groups = "drop")
    }, seed = TRUE)
  })

  # Resolve futures to collect results
  resolved_samples <- lapply(future_samples, value)
  
  # Combine all bootstrap results into one dataframe
  combined_results <- bind_rows(resolved_samples, .id = "replication")
  
  # Calculate the overall mean and standard error (SE) for each group
  results_future <- combined_results %>%
    group_by(teamID) %>%
    summarise(
      SE_RF = sd(mean_RF, na.rm = TRUE),  # Standard error from bootstrap samples
      .groups = "drop"
    )
  
  # Calculate the actual mean RF for each team using the full dataset
  actual_means <- fielding2 %>%
    group_by(teamID) %>%
    summarise(mean_RF = mean(RF, na.rm = TRUE), .groups = "drop")
  
  # Combine actual means and standard errors
  results_table_future <- actual_means %>%
    left_join(results_future, by = "teamID")

})

# Reset future plan
plan(sequential)

# Print timing and results
time_future
# results_table_future

```


### b. Generate a table showing the estimated RF and associated standard errors for the teams with the 10 highest RF from the three approaches.

Below we see the estimated RF and associated standard errors. The estimated standard errors were similar across all three approaches.

```{r}
# Add a column to each dataframe to indicate the method used
results_standard <- results_standard %>%
  mutate(Method = "Standard")

results_table_parallel <- results_table_parallel %>%
  mutate(Method = "Parallel")

results_table_futures <- results_table_future %>%
  mutate(Method = "Future")

# Combine all results into one dataframe
combined_results <- bind_rows(results_standard, results_table_parallel, results_table_futures)

# Order
combined_results <- combined_results %>%
  rename(
    `Estimated Mean RFs` = mean_RF,
    `Est SEs` = SE_RF
  ) %>%
  arrange(teamID)

# Get the top 10 teams by Estimated Mean RFs
top_10_teams <- combined_results %>%
  group_by(teamID) %>%
  # summarise so you can get everything down to 1 row/team, even though the avg is the same for all rows for a given team
  summarise(avg_RF = mean(`Estimated Mean RFs`, na.rm = TRUE)) %>%
  arrange(desc(avg_RF)) %>%
  head(10) %>%
  pull(teamID)

# Filter the combined results to include only the top 10 teams
top_10_combined_results <- combined_results %>%
  filter(teamID %in% top_10_teams) %>%
  arrange(desc(`Estimated Mean RFs`), teamID, Method)

# Set the maximum number of rows to print to Inf (infinite)
options(tibble.print_max = Inf)

# Print the filtered results
print(top_10_combined_results)

```

### c. Report and discuss the performance difference between the versions.

In the below Timing Results table, we see the performance differences between the versions. Although the Standard approach was expected to be the slowest because of how all computations were performed sequentially, it surprisingly performed faster than the Future approach but slower than the Parallel approach. As we talked about in class, the Standard approach is not as efficient for large datasets or when bootstrapping involves many iterations.

The Parallel approach was the quickest and most efficient. With parLapply in the parallel package, tasks are distributed evenly at the start and run to completion. If one worker takes longer, other workers might sit idle. However, this is still quicker than performing the computations sequentially in the Standard approach.

Interestingly, the Future approach was the slowest. Although the future package schedules tasks to optimize worker utilization, the overhead of initializing a large number of futures and transferring data to and from workers could have added significant delays. (I did go to see the professor during office hours, and he said that while there were no obvious coding errors, the lag could be due to the way lapply works or something with my computer specifically...) Overall, this suggests that for tasks when there may be a large overhead, the Future approach could become inefficient.

```{r}
# Timing Results
timing_results <- data.frame(
  Approach = c("Standard", "Parallel", "Future"),
  Total_Time = c(time_standard["elapsed"], time_parallel["elapsed"], time_future["elapsed"])
)

# Print timing results
print(timing_results)
```



